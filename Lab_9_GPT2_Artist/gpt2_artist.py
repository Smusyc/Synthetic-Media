# -*- coding: utf-8 -*-
"""GPT2_Artist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PMmaUD7Sdi3x5no5TjYLJnaLTie14Xsw
"""

!pip install git+https://github.com//allegroai/clearml

!pip install clearml-agent

! export MPLBACKEND=TkAg

from clearml import Task, Logger

# Commented out IPython magic to ensure Python compatibility.
# %env CLEARML_WEB_HOST=https://app.clear.ml
# %env CLEARML_API_HOST=https://api.clear.ml
# %env CLEARML_FILES_HOST=https://files.clear.ml
# %env CLEARML_API_ACCESS_KEY=PII6865ZQUVOMWNU3NYY
# %env CLEARML_API_SECRET_KEY=wZXjcAeQGT8AdD92Reuh7bDsOppveMXWaoWOJjknQFqOYkW1fo

#clearml-init

Task.set_credentials(
   api_host="https://api.clear.ml",
    web_host = "https://app.clear.ml",
    key="PII6865ZQUVOMWNU3NYY",
    secret = "wZXjcAeQGT8AdD92Reuh7bDsOppveMXWaoWOJjknQFqOYkW1fo"
)

task = Task.init(
    project_name='GPT2_Artist',
    task_name='TextGeneration',
    tags=['TextGeneration','transformers','GPT2'])

!clearml-agent daemon --queue default

!pip insatll -U accelerate
!pip install -U transformers

!pip install rouge_score evaluate datasets

import torch
device = "cuda" if torch.cuda.is_available() else "cpu"

!pip install random2
import random

from datasets import load_dataset, Dataset, DatasetDict
import numpy as np

check_dataset = True
num_train_epochs = 1

datasets = load_dataset("huggingartists/bob-dylan")#Cdjq вариант
train_percentage = 0.6
validation_percentage = 0.3
test_percentage = 0.1
array = datasets['train']['text']
random.shuffle(array)
#train, validation, test = np.split(datasets['train']['text'], [int(len(datasets['train']['text']) * train_percentage), int(len(datasets['train']['text']) * (train_percentage + validation_percetage ))])
train, validation, test = np.split(
    array,
    [int(len(datasets['train']['text'])*train_percentage), int(len(datasets['train']['text'])*(train_percentage + validation_percentage))]
)
datasets = DatasetDict(
    {
        'train': Dataset.from_dict({'text':list(train)}),
        'validation': Dataset.from_dict({'text':list(validation)}),
        'test': Dataset.from_dict({'text':list(test)})
    }
)

len(train), len(validation), len(test)

from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
import pathlib

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2", cache_dir=pathlib.Path('cache').resolve())
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
  return tokenizer(examples["text"])

#tokenized_datasets = datasets.map(
#    tokenize_function,
#    batched = True,
#    num_proc = 4,
#    remove_columns = ["text"]
#)

#tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=["text"])
tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])

block_size = int(tokenizer.model_max_length / 4)
def group_texts(examples):
  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
  total_length = len(concatenated_examples[list(examples.keys())[0]])
  total_length = (total_length//block_size) * block_size
  result = {
      k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
      for k, t in concatenated_examples.items()
  }
  result["labels"] = result["input_ids"].copy()
  return result

#result_datasets = tokenized_datasets.map(
#    group_texts,
#    batched = True,
#    batch_size = 1000,
#    num_proc=1,
#)

result_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=1,
)

import random
from transformers import Trainer, TrainingArguments

!apt install accelerate -U

from torch import nn
from transformers import Trainer

!apt install transformers[torch]

! pip install -U accelerate
! pip install -U transformers

num_train_epochs = 10
seed_data = random.randint(0,2**32-1)
"""
training_args = TrainingArguments(
    f"output",
    overwrite_output_dir=True,
    learning_rate=1.327e-4,
    weight_decay=0.01,
    num_train_epochs=num_train_epochs,
    seed=seed_data,
    logging_steps=100,
    evaluation_strategy="steps",
    eval_steps=100,
)
"""

training_args = TrainingArguments(
    f"output",
    overwrite_output_dir=True,
    evaluation_strategy = "epoch",
    learning_rate=1.372e-4,
    weight_decay=0.01,
    num_train_epochs=num_train_epochs,
    save_total_limit=10,
    save_strategy='epoch',
    save_steps=1,
    report_to=None,
    seed=seed_data,
    logging_steps=100,
    do_eval=True,
    eval_steps=100,
    load_best_model_at_end=True
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=result_datasets["train"],
    eval_dataset=result_datasets["validation"]
)

trainer.train()

trainer.evaluate(result_datasets["test"])

def song_generation(output_sequences):
  predictions = []
  generated_sequences = []

  max_repeat = 2

  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):
    generated_sequence = generated_sequence.tolist()
    text = tokenizer.decode(generated_sequence, clean_up_tokenization_space=True, skip_special_tokens =True)
    generated_sequences.append(text.strip())

  for i, g in enumerate(generated_sequences):
    res = str(g).replace('\n\n\n', '\n').replace('\n\n', '\n')
    lines = res.split('\n')
    i = max_repeat
    while i != len(lines):
      remove_count = 0
      for index in range(0, max_repeat):
        if lines[i - index - 1] == lines[i - index]:
          remove_count += 1
      if remove_count == max_repeat:
        lines.pop(i)
        i -= 1
      else:
        i += 1
    predictions.append('\n'.join(lines))

  return predictions

start_words = "How many"
num_sequences = 1
min_length = 100
max_length = 160
temperature = 1
top_p = 0.95
top_k = 50
repetition_penalty = 1.0

encoded_prompt = tokenizer(start_words, add_special_tokens = False, return_tensors="pt").input_ids
encoded_prompt = encoded_prompt.to(trainer.model.device)

output_sequences = trainer.model.generate(
                      input_ids=encoded_prompt,
                      max_length=max_length,
                      min_length=min_length,
                      temperature = float(temperature),
                      top_p=float(top_p),
                      top_k=int(top_k),
                      do_sample=True,
                      repetition_penalty = repetition_penalty,
                      num_return_sequences = num_sequences
                      )
text_song = song_generation(output_sequences)

text_song

"""
Сколько морей должна проплыть белая голубка,
прежде чем она уснет на песке?
Да, и сколько раз должны пролететь пушечные ядра,
прежде чем они будут навсегда запрещены?\

Ответ, мой друг, развеян по ветру,
ответ развеян по ветру,
и сколько лет может просуществовать гора,
прежде чем ее смоет в море?

Да, и сколько лет могут существовать некоторые люди,
прежде чем им позволят стать свободными?
Да, и сколько раз человек может повернуть голову,
делая вид, что он просто не видит?

\Ответ, мой друг, развеян по ветру
\Ответ развеян по ветру\
Нет, и сколько раз человек может посмотреть вверх,
прежде чем он сможет увидеть небо
"""

