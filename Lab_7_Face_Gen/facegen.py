# -*- coding: utf-8 -*-
"""FaceGen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ovvIrvjyu-R36uIuVxXeNem1Ayx-Ds4t

## Импорт
"""

!pip install ninja
!pip install imageio-ffmpeg
!pip install -U -q PyDrive
!pip install lpips
!pip install pytorch_msssim
!pip install pytorch-ignite

# Commented out IPython magic to ensure Python compatibility.

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
# This only needs to be done once per notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# %mkdir raw_images
!git clone https://github.com/NVlabs/stylegan2-ada-pytorch
# %cd stylegan2-ada-pytorch
!git clone https://github.com/denkogit/stylegan2_models


!mkdir pretrained_models
# %cd pretrained_models
!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl
!wget https://github.com/davisking/dlib-models/raw/master/shape_predictor_68_face_landmarks.dat.bz2
!bzip2 -d /content/stylegan2-ada-pytorch/pretrained_models/shape_predictor_68_face_landmarks.dat.bz2

file_id = '1cUv_reLE6k3604or78EranS7XzuVMWeO' # URL id.
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('e4e_ffhq_encode.pt')

# %cd ..

# ref https://github.com/Puzer/stylegan-encoder/blob/master/align_images.py
# !git clone https://github.com/omertov/encoder4editing

import pickle
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from stylegan2_models.e4e.model_utils import load_e4e_standalone
import torchvision.transforms as transforms

"""## Загрузка моделей"""

device = "cuda:0"
model_path = "/content/stylegan2-ada-pytorch/pretrained_models/ffhq.pkl"

with open(model_path, "rb") as f:
    face_generator = pickle.load(f)['G_ema'].cuda()
    #face_generator = pickle.load(f)['G_ema'].cpu()

e4e_model, _ = load_e4e_standalone("/content/stylegan2-ada-pytorch/pretrained_models/e4e_ffhq_encode.pt")

image2e4etensor = transforms.Compose([transforms.ToTensor(),
                                      transforms.Resize((256,256)),
                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])

"""## Генерация"""

import random
images = []
noise_mode = 'const'
c = 0

for i in range(10):
  seed = random.randint(64364, 4273462)

  z = torch.from_numpy(np.random.RandomState(seed).randn(1, face_generator.z_dim)).to(device)
  ws = face_generator.mapping(z, c, truncation_psi = (0.8))

  generated_tensor = face_generator.synthesis(ws, noise_mode=noise_mode, force_fp32=True)

  tensor = (generated_tensor.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)
  img = tensor[0].cpu().numpy()
  #img = tensor[0].cuda().numpy()

  images.append(img)

common_img = Image.fromarray(np.hstack((img for img in images)), 'RGB')

plt.imshow(common_img)

"""##Интерполяция"""

latents = []

for i in range(2):
  target_uint8 = np.array(images[i], dtype = np.uint8)
  e4e_tensor = image2e4etensor(target_uint8).to(device).unsqueeze(0)
  #latent_vector = e4e_model(e4e_tensor).detach().cpu()
  latent_vector = e4e_model(e4e_tensor).detach().cuda()
  latents.append(latent_vector)

def interpolate(face_generator, latent1, latent2, psi=0.5):
    latent1 = latent1.clone()
    latent2 = latent2.clone()
    indeces = [5,6,7,8,9,10,11,12]
    for i in indeces:
        latent1[:, i] = latent2[:, i].lerp(latent1[:, i], psi)

    generated_tensor = face_generator.synthesis(latent1, noise_mode='const', force_fp32=True)
    return generated_tensor

imgs = []
for i in [i/10 for i in range(-10, 10, 5)]:
  #interpolation = interpolate(face_generator.cpu(), latents[0], latents[1], psi=i)
  interpolation = interpolate(face_generator.cuda(), latents[0], latents[1], psi=i)
  tensor = (interpolation.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)
  img = tensor[0].cpu().numpy()
  #img = tensor[0].cuda().numpy()

  imgs.append(img)

concatted_img = Image.fromarray(np.hstack((img for img in imgs)), 'RGB')

plt.imshow(concatted_img)

"""## Замена лиц"""

!pip install lpips

from google.colab import drive
drive.mount('/content/drive')

import PIL
import os
import numpy as np
import torch
import lpips
import torch.nn as nn
import torchvision.transforms as transforms
from pytorch_msssim import ms_ssim
from lpips import LPIPS

device = "cuda:0"
class Rec_loss(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1_criterion = torch.nn.L1Loss(reduction='mean')


    def forward(self, target, synth):

        target = torch.add(target, 1.0)
        target = torch.mul(target, 127.5)
        target = target / 255

        synth = torch.add(synth, 1.0)
        synth = torch.mul(synth, 127.5)
        synth = synth / 255

        loss = torch.mean(1 - ms_ssim(synth, target, data_range=1, size_average=True))
        return loss


class Lpips_loss(nn.Module):
    def __init__(self, device):
        super().__init__()
        self.lpips_loss = LPIPS(net='vgg')
        self.lpips_loss.to(device)
        self.lpips_loss.eval()

    def forward(self, target, synth):
        return torch.mean(self.lpips_loss(target, synth))

def broadcast_w_sg(w_batch, cast_n=18):
    input_ws = []
    for w in w_batch:
        w_broadcast = torch.broadcast_to(w, (cast_n, 512))
        input_ws.append(w_broadcast)
    return torch.stack(input_ws)


def image2tensor_norm(image):
    transform = transforms.Compose([transforms.ToTensor(),
                                   transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])
    tensor = transform(image)
    return tensor

def load_image(img_path, img_size=1024):

  target_pil = PIL.Image.open(img_path).convert('RGB')
  target_pil = target_pil.resize((img_size, img_size))
  return target_pil



target_image = load_image('/content/drive/MyDrive/SyntheticMedia/Dataset/Man.jpg')
target_tensor = image2tensor_norm(target_image).cuda().unsqueeze(0)

lpips = Lpips_loss(device)
rec = Rec_loss()

def get_average_latent_space_vector(w_avg_samples = 10000, seed = 42):
  z_samples = torch.from_numpy(np.random.RandomState(seed).randn(w_avg_samples, face_generator.z_dim)). to(device)
  w_samples = face_generator.mapping(z_samples, None)
  w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)
  #w_samples = w_samples[:, :1, :].cuda().numpy().astype(np.float32)
  w_avg = np.mean(w_samples, axis=0, keepdims=True)
  w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5
  return w_avg

w_avg = get_average_latent_space_vector()
w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True)

optimizer = torch.optim.Adam([w_opt], lr = 0.05)

faces = []

seed = random.randint(64364, 4273462)
z = torch.from_numpy(np.random.RandomState(seed).randn(1, face_generator.z_dim)).to(device)
ws = face_generator.mapping(z, c=0, truncation_psi=(0.8))

generated_tensor = face_generator.synthesis(ws, noise_mode = 'const', force_fp32=True)
tensor = (generated_tensor.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)
#img = tensor[0].cuda().numpy()
img = tensor[0].cpu().numpy()
faces.append(img)

#!pip install tqdm
import tqdm

steps = 200

for step in tqdm.tqdm(range(steps)):
  synth_tensor = face_generator.synthesis(broadcast_w_sg(w_opt), noise_mode='const')

  lpips_loss = lpips(synth_tensor, target_tensor)
  rec_loss = rec(synth_tensor, target_tensor)

  loss = lpips_loss + rec_loss

  optimizer.step()
  optimizer.zero_grad(set_to_none=True)
  loss.backward()

generated_tensor = face_generator.synthesis(broadcast_w_sg(w_opt), noise_mode='const', force_fp32=True)
tensor = (generated_tensor.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0,255).to(torch.uint8)
faces.append(tensor[0].cpu().numpy())

faces.append(target_image)
concatted_img = Image.fromarray(np.hstack((img for img in faces)), 'RGB')

plt.imshow(concatted_img)

print(loss.item())

