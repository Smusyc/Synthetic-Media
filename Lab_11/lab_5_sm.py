# -*- coding: utf-8 -*-
"""Lab_5_SM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CEyb88FRhBzPlmvZ8Au_-BXhnZASr7il

##Задание 1##
"""

!pip install transformers

from google.colab import drive
drive.mount('/content/drive')
import cv2

video_capture = cv2.VideoCapture("./drive/MyDrive/data/cats_and_a_treadmill.mp4")

import os
os.makedirs("frames", exist_ok=True)

frame_count = 0

while video_capture.isOpened():
  ret, frame = video_capture.read()
  if not ret:
    break

  frame_filename = os.path.join("frames", f"frame_{frame_count:04d}.jpg")

  cv2.imwrite(frame_filename, frame)
  frame_count += 1

video_capture.release()
print(f"Извлечено {frame_count} кадров и сохранено в папку 'frames'.")

from transformers import pipeline

image_to_text = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning")

image_folder = "frames"
image_files = [file for file in os.listdir(image_folder) if file.endswith(".jpg")]

descriptions = []

for image_file in image_files:
  image_path = os.path.join(image_folder, image_file)

  result = image_to_text(image_path)
  descriptions.append(result)

descriptions_list = []

for description in descriptions:
  descriptions_list.append(description[0]['generated_text'])

import spacy
from collections import Counter

nlp = spacy.load("en_core_web_sm")

nouns = []
verbs = []

for description in descriptions_list:
  doc = nlp(description)

  for token in doc:
    if token.pos_ == "NOUN":
      nouns.append(token.text)
    elif token.pos_ == "VERB":
      verbs.append(token.text)

most_common_nouns = Counter(nouns).most_common(3)
most_common_verbs = Counter(verbs).most_common(3)

print("Наиболее часто встречающиеся существительные:")
for noun, count in most_common_nouns:
  print(f"{noun}: {count} раз")

print("\nНаиболее часто встречающиеся глаголы:")
for verb, count in most_common_verbs:
  print(f"{verb}: {count} раз")

"""##Задание 2##"""

!pip install fvcore

!pip install av

!pip install pytorchvideo

from typing import Dict
import json
import urllib
import torch
from torchvision.transforms import Compose, Lambda
from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo
from pytorchvideo.data.encoded_video import EncodedVideo
from pytorchvideo.transforms import ApplyTransformToKey, ShortSideScale, UniformTemporalSubsample, UniformCropVideo

#Загрузка моделей и меток классов
model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)

device = "cpu"
#device = "gpu"
model = model.eval()
model = model.to(device)

!wget https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json

json_filename = "/content/kinetics_classnames.json"

with open(json_filename, "r") as f:
  kinetics_classnames = json.load(f)

kinetics_id_to_classname = {}
for k, v in kinetics_classnames.items():
  kinetics_id_to_classname[v] = str(k).replace('"', "")

#Инициализация параметров и трансформация под модель
video_path = './drive/MyDrive/data/Rolling_cat.mp4'

#гиперпараметры
side_size = 256
mean = [0.45, 0.45, 0.45]
std = [0.225, 0.225, 0.225]
crop_size = 256
num_frames = 32
sampling_rate = 2
frames_per_second = 30
slowfast_alpha = 4
num_clips = 10
num_crops = 3

#Переопределение класса для трансформации кадров в видео
class PackPathway(torch.nn.Module):
  #Трансформация кадров видео в тензоры
  def __init__(self):
    super().__init__()

  def forward(self, frames: torch.Tensor):
    fast_pathway = frames
    slow_pathway = torch.index_select(
        frames,
        1,
        torch.linspace(
            0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha
        ).long()
    )
    frame_list = [slow_pathway, fast_pathway]
    return frame_list

transform = ApplyTransformToKey(
    key="video",
    transform=Compose(
        [
            UniformTemporalSubsample(num_frames),
            Lambda(lambda x: x/255.0),
            NormalizeVideo(mean, std),
            ShortSideScale(
                size=side_size
            ),
            CenterCropVideo(crop_size),
            PackPathway()
        ]
    ),
)

#длина клипа на основе гиперпараметров
clip_duration = (num_frames * sampling_rate)/frames_per_second

#Генерация входных данных и получение предсказаний
start_sec = 0
end_sec = start_sec + clip_duration

video = EncodedVideo.from_path(video_path)

video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)

video_data = transform(video_data)

inputs = video_data["video"]
inputs = [i.to(device)[None, ...] for i in inputs]

preds = model(inputs)

post_act = torch.nn.Softmax(dim=1)
preds = post_act(preds)
pred_classes = preds.topk(k=10).indices[0]
pred_classes

pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]

print("Топ 10 предсказаний: %s" % ", ".join(pred_class_names))